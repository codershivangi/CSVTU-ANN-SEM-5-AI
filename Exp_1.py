# -*- coding: utf-8 -*-
"""Exp_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m6jw8zOShQCCecT8oEs3HxOFjXLcnUCe
"""

# Perceptron implementation for AND gate

# Training data (AND gate)
# Inputs: x1, x2
# Output: y
training_data = [
    ([0, 0], 0),
    ([0, 1], 0),
    ([1, 0], 0),
    ([1, 1], 1)
]

# Initialize weights and bias
weights = [0.0, 0.0]
bias = 0.0
learning_rate = 0.1
epochs = 10

# Step activation function
def step_function(value):
    if value >= 0:
        return 1
    else:
        return 0

# Training the perceptron
for epoch in range(epochs):
    print(f"Epoch {epoch+1}")
    for inputs, target in training_data:
        # Weighted sum
        net_input = inputs[0] * weights[0] + inputs[1] * weights[1] + bias

        # Prediction
        prediction = step_function(net_input)

        # Error
        error = target - prediction

        # Update weights and bias
        weights[0] += learning_rate * error * inputs[0]
        weights[1] += learning_rate * error * inputs[1]
        bias += learning_rate * error

        print(f"Inputs: {inputs}, Target: {target}, Prediction: {prediction}")

print("\nFinal Weights:", weights)
print("Final Bias:", bias)